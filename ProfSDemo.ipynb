{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain openai faiss-cpu sentence-transformers pypdf tiktoken"
      ],
      "metadata": {
        "id": "9cxiPIq9vE1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KMveziAdaXMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBJYZMg1q6UC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "PAPER_DIR   = \"/content/drive/MyDrive/ProfS/ProfS_Papers\"\n",
        "INDEX_PATH  = \"/content/drive/MyDrive/ProfS/ProfS_FAISS\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq langchain-community pypdf"
      ],
      "metadata": {
        "id": "d-Ju8qp0CA3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 · load + chunk papers\n",
        "import glob, os\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "pdf_files = glob.glob(os.path.join(PAPER_DIR, \"**/*.pdf\"), recursive=True)\n",
        "docs = []\n",
        "for pdf in pdf_files:\n",
        "    docs += PyPDFLoader(pdf).load()\n",
        "\n",
        "print(f\"Loaded {len(pdf_files)} PDFs  →  {len(docs)} pages\")\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "chunks   = splitter.split_documents(docs)\n",
        "print(f\"Split into {len(chunks):,} chunks\")"
      ],
      "metadata": {
        "id": "VLYj35KLssz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 · embed with small model + build / load FAISS index\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from pathlib import Path\n",
        "\n",
        "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# tag every PDF chunk so we can recognise genuine sources\n",
        "for c in chunks:\n",
        "    c.metadata[\"type\"] = \"paper\"\n",
        "\n",
        "# (re)build\n",
        "vectordb = FAISS.from_documents(chunks, emb)\n",
        "vectordb.save_local(INDEX_PATH)\n",
        "print(\"FAISS index rebuilt with latest PDFs ✔️\")\n",
        "\n",
        "# (lower → stricter)\n",
        "retriever = vectordb.as_retriever(\n",
        "    search_kwargs={\"k\": 4, \"score_threshold\": 0.75}\n",
        ")"
      ],
      "metadata": {
        "id": "U8kEPFZfsuBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3\n",
        "from langchain.prompts.chat import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Define system prompt\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are Professor S, a kind, patient, slightly quirky CS professor. \"\n",
        "    \"You often say 'emm…', 'actually', 'fascinating', 'oh I see', and begin with \"\n",
        "    \"'Fantastic question!'. End every answer with ':D '. \"\n",
        "    \"Use the following context to inform your answer. If the context is relevant, prioritize it. \"\n",
        "    \"If the question is unrelated to the context, use your general knowledge to provide a helpful answer.\"\n",
        "    \"\\n\\nContext:\\n{context}\"\n",
        ")\n",
        "\n",
        "\n",
        "# Prompt structure using system + memory + question\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(SYSTEM_PROMPT),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "])\n",
        "\n",
        "CONDENSE_Q_SYSTEM = (\n",
        "    \"You are a helpful assistant. You get the prior chat_history \"\n",
        "    \"(as a list of messages) plus the user's follow-up question. \"\n",
        "    \"Rewrite the follow-up so it can stand alone.\"\n",
        ")\n",
        "\n",
        "condense_question_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(CONDENSE_Q_SYSTEM),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),   # list is fine\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "])\n",
        "\n",
        "# LLM and memory\n",
        "import os, getpass\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4o-2024-08-06\",\n",
        "    temperature=0.8,\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
      ],
      "metadata": {
        "id": "kJQ1D1las0ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4\n",
        "import json, pathlib\n",
        "from langchain.memory.chat_message_histories import ChatMessageHistory\n",
        "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
        "\n",
        "MEM_FILE = f\"{PAPER_DIR}/profS_memory.jsonl\"\n",
        "\n",
        "def load_memory():\n",
        "    if not pathlib.Path(MEM_FILE).exists():\n",
        "        return []\n",
        "    with open(MEM_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [json.loads(ln) for ln in f]\n",
        "\n",
        "def append_memory(msgs):\n",
        "    with open(MEM_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "        for m in msgs:\n",
        "            f.write(json.dumps(m, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# Load and wrap messages\n",
        "long_term_history = load_memory()\n",
        "print(f\"Loaded {len(long_term_history)//2} previous Q&A pairs\")\n",
        "\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain.memory.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "chat_history = ChatMessageHistory()\n",
        "for m in long_term_history:\n",
        "    role = m[\"role\"]\n",
        "    content = m[\"content\"]\n",
        "    if role == \"user\":\n",
        "        chat_history.add_user_message(content)\n",
        "    elif role == \"assistant\":\n",
        "        chat_history.add_ai_message(content)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown role: {role}\")\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    chat_memory=chat_history,\n",
        "    output_key=\"answer\",           # tell memory which field to store\n",
        ")\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "# build RAG chain\n",
        "rag = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    get_chat_history=lambda hist: hist,          # keep list structure\n",
        "    condense_question_prompt=condense_question_prompt,\n",
        "    combine_docs_chain_kwargs={\n",
        "        \"prompt\": prompt,\n",
        "        \"document_variable_name\": \"context\",\n",
        "    },\n",
        "    return_source_documents=True,                # sources\n",
        "\n",
        "    verbose=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "SOq9TkaQs4f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 · Chat loop\n",
        "from langchain.schema import Document, SystemMessage, HumanMessage\n",
        "from datetime import datetime\n",
        "import textwrap\n",
        "\n",
        "def wrap(txt: str) -> str:\n",
        "    return textwrap.fill(txt, 90)\n",
        "\n",
        "print(\"Ask Professor S anything :D  (type 'exit' to quit)\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        q = input(\"\\nStudent: \").strip()\n",
        "        if q.lower() in {\"exit\", \"quit\"}:\n",
        "            break\n",
        "\n",
        "        # 1 · RAG\n",
        "        result  = rag.invoke({\"question\": q})\n",
        "        answer  = result[\"answer\"]\n",
        "        # keep only chunks that really came from PDFs\n",
        "        sources = [d for d in result.get(\"source_documents\", [])\n",
        "                   if d.metadata.get(\"type\") == \"paper\"]\n",
        "\n",
        "        # 2 · Decide if fallback is needed\n",
        "        use_fallback = len(sources) == 0\n",
        "        if use_fallback:\n",
        "            print(\"(no relevant paper context → fallback to GPT-4o)\")\n",
        "            completion = llm.invoke([\n",
        "                SystemMessage(\n",
        "                    content=\"You are Professor S, a kind, patient, slightly quirky CS professor. \"\n",
        "                            \"End every answer with ':D '\"),\n",
        "                HumanMessage(content=q)\n",
        "            ])\n",
        "            answer = completion.content.strip()\n",
        "\n",
        "        # 3 · Display\n",
        "        if not use_fallback:\n",
        "            print(\"\\nRetrieved context from papers:\")\n",
        "            for d in sources:\n",
        "                print(\"•\", d.page_content[:150].replace(\"\\n\", \" \"), \"…\")\n",
        "        print(\"\\nProfessor S:\", wrap(answer))\n",
        "\n",
        "        # 4 · Append to long-term memory file\n",
        "        append_memory([\n",
        "            {\"role\": \"user\",      \"content\": q},\n",
        "            {\"role\": \"assistant\", \"content\": answer}\n",
        "        ])\n",
        "\n",
        "        # 5 · grow the vector store **only** with RAG answers\n",
        "        if not use_fallback and sources:\n",
        "            ts = datetime.utcnow().isoformat()\n",
        "            vectordb.add_documents([\n",
        "                Document(page_content=q,      metadata={\"role\": \"user\",      \"ts\": ts}),\n",
        "                Document(page_content=answer, metadata={\"role\": \"assistant\", \"ts\": ts})\n",
        "            ])\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nSession interrupted.\")\n",
        "finally:\n",
        "    vectordb.save_local(INDEX_PATH)\n",
        "    print(\"Index saved to\", INDEX_PATH)\n"
      ],
      "metadata": {
        "id": "yMiY0aWWs7H4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}